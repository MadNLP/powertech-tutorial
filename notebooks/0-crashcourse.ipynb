{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Crash-course\n",
    "This first part is devoted to learning the basic of GPU programming.\n",
    "For those not familiar with the Julia language, we highly recommend\n",
    "reading [this introduction](https://jump.dev/JuMP.jl/stable/tutorials/getting_started/getting_started_with_julia/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPU programming using CUDA.jl\n",
    "Julia has an excellent support for GPU programming with the package CUDA.jl.\n",
    "We recommend [this tutorial](https://cuda.juliagpu.org/stable/tutorials/introduction/)\n",
    "to understand the basic concept for programming on the GPU. Once installed,\n",
    "you can import CUDA as:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using CUDA"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default, Julia allocates a new array on the host:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x_cpu = zeros(10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The allocation of a new vector on the GPU has to be explicited as"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x_gpu = CUDA.zeros(10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default, CUDA.jl allocates a vector of `float`. In scientific computing, it\n",
    "is often recommended to work with `double`, encoded by the type `Float64` in Julia:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x_gpu = CUDA.zeros(Float64, 10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The array can be manipulated using the broadcast operator (using a syntax similar as in matlab).\n",
    "Incrementing all the elements in `x_gpu` by 1 just amounts to"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x_gpu .+= 1.0"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the GPU, accessing the element of an array by its index (e.g. calling `x_gpu[1]`)\n",
    "is prohibited by default, and should by avoided\n",
    "at all cost. The whole point of using a GPU is to evaluate things in parallel, so it usually makes little\n",
    "sense to access an array element by element. If you have to implement non-trivial operations with complicated indexing, you have to resort to implementing a custom GPU kernels by yourself. In general,\n",
    "\n",
    "- We recommend using the broadcast operator `.` as much as you can, as it generates automatically the GPU kernels you need to implement the operation.\n",
    "- If you really have to, you can implement your own GPU kernel [using CUDA.jl](https://cuda.juliagpu.org/stable/tutorials/introduction/#Writing-your-first-GPU-kernel) or using an abstraction layer like [KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl/).\n",
    "\n",
    "Now comes the question of evaluating complicated expressions on the GPU."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling with ExaModels.jl\n",
    "In optimization, it is recommended to use a modeler that acts as a domain specific language providing you all the syntax needed to implement your optimization problem. [Ampl](ampl.com), [JuMP.jl](https://jump.dev) and [Pyomo](https://www.pyomo.org/) are among the most popular modelers, but none of them support GPUs.\n",
    "[ExaModels.jl](https://exanauts.github.io/ExaModels.jl/dev/) is an attempt to fill this gap.\n",
    "As for CUDA.jl, we recommend [this introductory course](https://exanauts.github.io/ExaModels.jl/dev/guide/) to ExaModels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can import ExaModels.jl simply as"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ExaModels"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "and instantiate a new model with"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "core = ExaCore()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding new variables to the model is very much similar to other modelers. E.g., adding 10 lower-bounded\n",
    "variables $x ≥ 0$ amounts to"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = variable(core, 10; lvar=0.0)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the variables defined, ExaModels relies on [a powerful SIMD abstraction](https://exanauts.github.io/ExaModels.jl/dev/simd/) to identify\n",
    "automatically the potential for parallelism in the expression tree.\n",
    "ExaModels implements the expression trees using iterator objects. Like programming on GPU,\n",
    "we should define all the expression in iterator format, and we should avoid accessing the variable `x`\n",
    "by its index outside a generator.\n",
    "\n",
    "As a demonstration, we show how to generate the constraint $10 × sin(x_i) ≥ 0$.\n",
    "We start by building a Julia generator encoding the expression:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gen = (10.0 * sin(x[i]) + i for i in 1:10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can pass the generator `gen` to ExaModels to build our inequality constraint:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cons = constraint(core, gen; lcon=0.0)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the generator does not evaluate the expression, but just provide a way to generate it.\n",
    "The evaluation part comes apart, by creating an `ExaModel` instance that takes as input\n",
    "the structure `core` that stores all the expressions used to generate the model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nlp = ExaModel(core)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The constructor `ExaModel` generates an `AbstractNLPModel`, which comes with a proper API\n",
    "to evaluate the model in a syntax appropriate for numerical optimization.\n",
    "The API can be found [in this documentation](https://jso.dev/NLPModels.jl/stable/api/#Reference-guide).\n",
    "As a consequence, evaluating the constraints implemented by the generator we defined before just\n",
    "translates to:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using NLPModels\n",
    "x = ones(10)  # get an initial point\n",
    "c = NLPModels.cons(nlp, x)  # return the results as a vector"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As ExaModels is just manipulating expression, it is very easy to offload the evaluation of the model on the GPU: it just requires to build the appropriate kernels to evaluate the expressions implemented in the generators, a task performed automatically by ExaModels.\n",
    "You can generate a new model on the GPU simply by specifying a new backend to ExaModels:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "core = ExaCore(; backend=CUDABackend())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Afterwards, the generation of the model remains the same:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = variable(core, 10; lvar=0.0)\n",
    "cons = constraint(core, 10.0 * sin(x[i]) + i for i in 1:10; lcon=0.0)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And the evaluation of the model follows exactly the same syntax:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nlp = ExaModel(core)\n",
    "x_gpu = CUDA.ones(Float64, 10)  # get an initial point\n",
    "c_gpu = NLPModels.cons(nlp, x_gpu)  # return the results as a vector"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we will see in the next tutorial, ExaModels is a powerful tool to evaluate the\n",
    "model's derivatives using automatic differentiation. This will prove to be particularly\n",
    "useful for solving the power flow equations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.5",
   "language": "julia"
  }
 },
 "nbformat": 4
}
